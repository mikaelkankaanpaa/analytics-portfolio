{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c55095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_mdl_acc(model, x_data, y_data):\n",
    "    y_likelihoods = model.predict(x_data, verbose=0)\n",
    "    y_predictions = np.argmax(y_likelihoods, axis=1)\n",
    "    res = y_data - y_predictions\n",
    "    return sum(x == 0 for x in res) / y_data.shape[0]\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the input images from 0-255 to 0-1 range \n",
    "# (improves training stability and convergence, and is standard practice in DL;\n",
    "# Model seems to get stuck without normalization)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the image samples from 28x28 to 1x784\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) # 60000x784 training data matrix\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) # 10000x784 test data matrix\n",
    "\n",
    "# One-hot encode the class ids \n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Build & compile the model\n",
    "model = Sequential()\n",
    "# Using Input object instructed at https://keras.io/guides/sequential_model/:\n",
    "model.add(Input(shape=(784,))) # not an actual layer, just predefines the input shape for weights\n",
    "# see note 1\n",
    "model.add(Dense(64, activation='sigmoid')) # input layer with X neurons <------------- CHANGE FOR TESTING\n",
    "model.add(Dense(10, activation='sigmoid')) # output layer with 10 neurons (one per class)\n",
    "learning_rate = 0.5  # <-------------------------------------------------------------- CHANGE FOR TESTING\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "# Train the model with training data\n",
    "epochs = 30 # <----------------------------------------------------------------------- CHANGE FOR TESTING\n",
    "history = model.fit(x_train, y_train_one_hot, epochs=epochs, verbose=1)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training Loss (MSE) over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Evaluate the model accuracy on training & test data; see note 2\n",
    "acc_tr = evaluate_mdl_acc(model, x_train, y_train)\n",
    "print(f\"Classification accuracy with training data is {acc_tr * 100:.2f} %\")\n",
    "acc_te = evaluate_mdl_acc(model, x_test, y_test)\n",
    "print(f\"Classification accuracy with test data is {acc_te * 100:.2f} %\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Note 1: *******************************************************************************\n",
    "# Other activation methods in the input and output layers give better results; \n",
    "# e.g. with epochs = 20; learning_rate = 0.5 and\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(10, activation='softmax'));\n",
    "# ---> Classification accuracy with training data is 98.50 %;\n",
    "#      Classification accuracy with test data is 97.16 %\n",
    "# Changing the loss function to something else, sa. categorical_crossentropy might also help\n",
    "# https://keras.io/api/losses/\n",
    "\n",
    "# Note 2: *******************************************************************************\n",
    "# Can also be done by including the 'accuracy' metric in model.compile() @ln43, e.g.\n",
    "# 'model.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])' and later calling\n",
    "# model.evaluate(), e.g. 'model.evaluate(model.evaluate(x_train, y_train_one_hot, verbose=0))'\n",
    "# which would return the training data loss and accuracy directly after compiling\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
